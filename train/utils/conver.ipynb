{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_openrlhf_sft_data_json(dataset_dir,output_dir):\n",
    "    path = Path(dataset_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    files = [file.name for file in path.glob(\"*.json\")]\n",
    "    for file in files:\n",
    "        data_file = os.path.join(path, file)\n",
    "        raw_dataset = load_dataset(\"json\", data_files=data_file,keep_in_memory=False)\n",
    "        dataset = raw_dataset['train']\n",
    "        output = []\n",
    "        for i in range(len(dataset)):\n",
    "            data = dataset[i]\n",
    "            new_data = defaultdict(list)\n",
    "            new_data[\"prompt\"].append(\n",
    "                {\"role\":\"user\",\"content\":data[\"instruction\"] + \"\\n\" + data[\"input\"] if data[\"input\"] is not None else data[\"instruction\"]} \n",
    "            )\n",
    "            new_data[\"prompt\"].append(\n",
    "                {\"role\":\"assistant\",\"content\":data[\"output\"]}\n",
    "            )\n",
    "            output.append(new_data)\n",
    "        output_file = os.path.join(output_path, file.replace(\".json\", \"openrlhf.json\"))\n",
    "        with open(output_file, 'w',encoding='utf-8') as f:\n",
    "            f.write(json.dumps(output, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_openrlhf_rw_data_json(dataset_dir,output_dir):\n",
    "    path = Path(dataset_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    files = [file.name for file in path.glob(\"*.json\")]\n",
    "    for file in files:\n",
    "        data_file = os.path.join(path, file)\n",
    "        raw_dataset = load_dataset(\"json\", data_files=data_file,keep_in_memory=False)\n",
    "        dataset = raw_dataset['train']\n",
    "        output = []\n",
    "        for i in range(len(dataset)):\n",
    "            data = dataset[i]\n",
    "            new_data = defaultdict(list)\n",
    "            new_data[\"chosen\"].append(\n",
    "                {\"role\":\"user\",\"content\":data[\"question\"]} \n",
    "            )\n",
    "            new_data[\"chosen\"].append(\n",
    "                {\"role\":\"assistant\",\"content\":data[\"response_chosen\"]}\n",
    "            )\n",
    "            new_data[\"rejected\"].append(\n",
    "                {\"role\":\"user\",\"content\":data[\"question\"]} \n",
    "            )\n",
    "            new_data[\"rejected\"].append(\n",
    "                {\"role\":\"assistant\",\"content\":data[\"response_rejected\"]}\n",
    "            )\n",
    "            output.append(new_data)\n",
    "        output_file = os.path.join(output_path, file.replace(\".json\", \"openrlhf.json\"))\n",
    "        with open(output_file, 'w',encoding='utf-8') as f:\n",
    "            f.write(json.dumps(output, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_llamafactory_data_json(dataset_dir,output_dir):\n",
    "    path = Path(dataset_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    files = [file.name for file in path.glob(\"*.json\")]\n",
    "    for file in files:\n",
    "        data_file = os.path.join(path, file)\n",
    "        raw_dataset = load_dataset(\"json\", data_files=data_file,keep_in_memory=False)\n",
    "        dataset = raw_dataset['train']\n",
    "        output = []\n",
    "        for i in range(len(dataset)):\n",
    "            output.append(dataset[i])\n",
    "        output_file = os.path.join(output_path, file.replace(\".json\", \"_llamafactory.json\"))\n",
    "        with open(output_file, 'w',encoding='utf-8') as f:\n",
    "            f.write(json.dumps(output, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_llamafactory_data_json_rw(dataset_dir,output_dir):\n",
    "    path = Path(dataset_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    files = [file.name for file in path.glob(\"*.json\")]\n",
    "    for file in files:\n",
    "        data_file = os.path.join(path, file)\n",
    "        raw_dataset = load_dataset(\"json\", data_files=data_file,keep_in_memory=False)\n",
    "        dataset = raw_dataset['train']\n",
    "        output = []\n",
    "        for i in range(len(dataset)):\n",
    "            output.append({\"instruction\":dataset[i][\"question\"],\"input\":\"\",\"chosen\":dataset[i][\"response_chosen\"],\n",
    "                           \"rejected\":dataset[i][\"response_rejected\"]})\n",
    "        output_file = os.path.join(output_path, file.replace(\".json\", \"_llamafactory.json\"))\n",
    "        with open(output_file, 'w',encoding='utf-8') as f:\n",
    "            f.write(json.dumps(output, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/processing_data/infra/lvjiahui/study/LLM/RLHF/dataset/medical/reward\"\n",
    "output_dir = \"/processing_data/infra/lvjiahui/study/LLM/RLHF/dataset/medical/reward_llama_factory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_llamafactory_data_json_rw(dataset_dir,output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/processing_data/infra/lvjiahui/study/LLM/RLHF/dataset/medical/reward\"\n",
    "output_dir = \"/processing_data/infra/lvjiahui/work/datasets/dpo_rlhf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 302011 examples [00:08, 34719.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\",data_files=\"/processing_data/infra/lvjiahui/work/datasets/dpo_data/dpo_llamafactory.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 302011\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset['train']\n",
    "output = []\n",
    "for i in range(len(dataset1)):\n",
    "    data = dataset1[i]\n",
    "    new_data = defaultdict(list)\n",
    "    new_data[\"prompt\"].append(\n",
    "        {\"role\":\"user\",\"content\":data[\"prompt\"]} \n",
    "    )\n",
    "    output.append(new_data)\n",
    "output_file = os.path.join(output_dir, \"train_ppo.json\")\n",
    "with open(output_file, 'w',encoding='utf-8') as f:\n",
    "    f.write(json.dumps(output, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']\n",
    "output = []\n",
    "for i in range(len(dataset)):\n",
    "    data = dataset[i]\n",
    "    new_data = defaultdict(list)\n",
    "    new_data[\"chosen\"].append(\n",
    "        {\"role\":\"user\",\"content\":data[\"prompt\"]} \n",
    "    )\n",
    "    new_data[\"chosen\"].append(\n",
    "        {\"role\":\"assistant\",\"content\":data[\"chosen\"]}\n",
    "    )\n",
    "    new_data[\"rejected\"].append(\n",
    "        {\"role\":\"user\",\"content\":data[\"prompt\"]} \n",
    "    )\n",
    "    new_data[\"rejected\"].append(\n",
    "        {\"role\":\"assistant\",\"content\":data[\"rejected\"]}\n",
    "    )\n",
    "    output.append(new_data)\n",
    "output_file = os.path.join(output_dir, \"safe_rlhf.json\")\n",
    "with open(output_file, 'w',encoding='utf-8') as f:\n",
    "    f.write(json.dumps(output, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
